[15:52, 11/11/2025] Pietro: there is some amount of irreducible uncertainty (e.g. why do we train with only a few tokens worth of context, isn't that usually an impossiblr task?), and sometimes multiple options are valid. How can this be incorporated into the loss? something like an oracle, and if it fails, then the next token is truly unpredictable. or maybe have so that we smooth out the ref log probs so that they are not just one-hot encoded. but hjow/???
[16:09, 11/11/2025] Pietro: how hacky is reasoning? non-reasoning transformers are silly, since the amount of compute they can use is purely a function of how long the answer is (meaning that Repeat after me: "Hello Hello Hello" uses more compute than "Is the riemann hyptohesis true? yes or no". In humans, reasoning and language happen separately (in the snese that i dont spell out each of the mental steps i do), but obviosuyl there is a (in my opinion, fairly bulky) shared trunk ebtween the 2. So CoT is an approcximation of this latent reasoning (how good of an apprixmiation? depends on the size of the trunk i guess?). But suppose we wanted to find some more elegant paradigm (thjese consideratiosn are disjoint from more practicat things e.g. hwo it's nice that we have reasoning data to SFT on). One could be allow the model as much compute as it wants to emit the next token(see universal transformers). This is not terrible, but a problem is that once it produces it, all the information that is related to that reasoning process disappears : (. Another thing is have the model emit tuples of (reasoning, verbal) tokens, so the model can attend to both it's thinking process as well as its verbal output. This is better but also not great, since we might want to encode more information for some parts of the process. So i guess a good solution is to have the model output at will either a reasoning token, or a verbal token, and you can attend to both. Then just take the loss over the verbal tokens. Basically the model can produce it's own loss mask. PROBLEM: we have no sample data for this model


what is this weird training / inference mispath??? if you start generating autoregressively, and leave the thinking tokens in, the model is going to freak out since it has never seen those. So how do we get out of this?? -> reframe SFT as RL: then the reward the model gets is: did you predict the first non-masked token correctly??


Auto-regression is kinda crazy! all the reasoning that the model has is squished through the token space. Models when learning have some freeedom (e.g. you can still output the same tokens withj (approx) the same probabilty but have a different internal representation that makes it slightly easier to reason about the problem. I'd assum that is wqhat they are doing).

So where is information stored? [text](https://x.com/repligate/status/1965960676104712451)